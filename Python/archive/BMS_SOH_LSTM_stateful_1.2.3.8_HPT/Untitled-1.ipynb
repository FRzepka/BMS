{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866cf0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.amp import GradScaler, autocast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import csv\n",
    "import math\n",
    "import itertools\n",
    "import optuna\n",
    "import pickle  \n",
    "\n",
    "# Konstanten\n",
    "SEQ_CHUNK_SIZE = 4096    # Länge der Zeitreihen-Chunks für Seq-to-Seq\n",
    "\n",
    "# Gerät auswählen und cuDNN optimieren\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Datenlade-Funktion\n",
    "def load_cell_data(data_dir: Path):\n",
    "    dataframes = {}\n",
    "    for folder in sorted(data_dir.iterdir()):\n",
    "        if folder.is_dir() and folder.name.startswith(\"MGFarm_18650_\"):\n",
    "            dfp = folder / \"df.parquet\"\n",
    "            if dfp.exists():\n",
    "                dataframes[folder.name] = pd.read_parquet(dfp)\n",
    "            else:\n",
    "                print(f\"Warning: {dfp} fehlt\")\n",
    "    return dataframes\n",
    "\n",
    "# Daten vorbereiten\n",
    "def load_data(base_path: str = \"/home/users/f/flo01010010/HPC_projects/5_Data/MGFarm_18650_Dataframes\"):\n",
    "    base = Path(base_path)\n",
    "    cells = load_cell_data(base)\n",
    "    # neue Trainingszellen und feste Validierungszelle\n",
    "    train_cells = [\n",
    "        f\"MGFarm_18650_C{str(i).zfill(2)}\"\n",
    "        for i in [1,3,5,9,11,13,15,17,19,21,23,25,27]\n",
    "    ]\n",
    "    val_cell = \"MGFarm_18650_C07\"\n",
    "    # Feature-Liste nur Spannung, Strom und Q_m\n",
    "    feats = [\"Voltage[V]\", \"Current[A]\", \"Q_m\"]\n",
    "\n",
    "    # trainings-Daten initial (nur timestamp ergänzen)\n",
    "    train_dfs = {}\n",
    "    for name in train_cells:\n",
    "        df = cells[name].copy()\n",
    "        df['timestamp'] = pd.to_datetime(df['Absolute_Time[yyyy-mm-dd hh:mm:ss]'])\n",
    "        train_dfs[name] = df\n",
    "\n",
    "    # scaler auf *allen* Zellen fitten (nicht nur Training)\n",
    "    df_all = pd.concat(cells.values(), ignore_index=True)\n",
    "    scaler = MaxAbsScaler().fit(df_all[feats])\n",
    "    print(\"[INFO] Skaler über alle Zellen fitten\")\n",
    "\n",
    "    # Skalierte Trainingsdaten\n",
    "    train_scaled = {}\n",
    "    for name, df in train_dfs.items():\n",
    "        df2 = df.copy()\n",
    "        df2[feats] = scaler.transform(df2[feats])\n",
    "        train_scaled[name] = df2\n",
    "    # debug: check for NaNs after scaling\n",
    "    for name, df2 in train_scaled.items():\n",
    "        nan_counts = pd.DataFrame(df2[feats]).isna().sum().to_dict()\n",
    "        print(f\"[DEBUG] {name} NaNs after train scaling:\", {k:v for k,v in nan_counts.items() if v>0} or \"none\")\n",
    "\n",
    "    # Validierung/Test der dritten Zelle\n",
    "    df3 = cells[val_cell].copy()\n",
    "    df3['timestamp'] = pd.to_datetime(df3['Absolute_Time[yyyy-mm-dd hh:mm:ss]'])\n",
    "    L = len(df3)\n",
    "    # 50% der Zelle für Val, die nächsten 10% für Test\n",
    "    i1, i2 = int(L * 0.5), int(L * 0.6)\n",
    "    df_val  = df3.iloc[:i1].copy()\n",
    "    df_test = df3.iloc[i1:i2].copy()\n",
    "    df_val[feats]  = scaler.transform(df_val[feats])\n",
    "    df_test[feats] = scaler.transform(df_test[feats])\n",
    "    # debug: shapes & NaNs in val/test\n",
    "    print(f\"[DEBUG] df_val length: {len(df_val)}, df_test length: {len(df_test)}\")\n",
    "    print(\"[DEBUG] df_val NaNs:\", df_val[feats].isna().sum().to_dict())\n",
    "    print(\"[DEBUG] df_test NaNs:\", df_test[feats].isna().sum().to_dict())\n",
    "\n",
    "    return train_scaled, df_val, df_test, train_cells, val_cell, scaler\n",
    "\n",
    "# Angepasstes Dataset für ganze Zellen\n",
    "class CellDataset(Dataset):\n",
    "    def __init__(self, df, sequence_length=SEQ_CHUNK_SIZE):\n",
    "        \"\"\"Dataset für eine ganze Zelle, aufgeteilt in Sequenz-Chunks\"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.data   = df[[\"Voltage[V]\", \"Current[A]\", \"Q_m\"]].values\n",
    "        self.labels = df[\"SOH_ZHU\"].values\n",
    "        self.n_batches = max(1, len(self.data) // self.sequence_length)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_batches  # Anzahl der Batches\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.sequence_length\n",
    "        end = min(start + self.sequence_length, len(self.data))\n",
    "        x = torch.from_numpy(self.data[start:end]).float()\n",
    "        y = torch.from_numpy(self.labels[start:end]).float()\n",
    "        return x, y\n",
    "\n",
    "# Weight-initialization helper\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.LSTM):\n",
    "        for name, p in m.named_parameters():\n",
    "            if 'weight_ih' in name or 'weight_hh' in name:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(p, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# Modell: LSTM + Dropout + MLP-Head\n",
    "def build_model(input_size=4, hidden_size=32, num_layers=1, dropout=0.2, mlp_hidden=32):\n",
    "    class SOHModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            # LSTM ohne Dropout (voller Informationsfluss)\n",
    "            self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                                batch_first=True, dropout=0.0)\n",
    "            # hidden_size bestimmt die Dim. der LSTM-Ausgabe\n",
    "            # mlp_hidden ist die Größe der verborgenen MLP-Schicht\n",
    "            self.mlp = nn.Sequential(\n",
    "                nn.Linear(hidden_size, mlp_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),   # nur hier Dropout\n",
    "                nn.Linear(mlp_hidden, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        def forward(self, x, hidden):\n",
    "            self.lstm.flatten_parameters()       # cuDNN-ready\n",
    "            x = x.contiguous()                   # ensure input contiguous\n",
    "            # make hidden states contiguous\n",
    "            h, c = hidden\n",
    "            h, c = h.contiguous(), c.contiguous()\n",
    "            hidden = (h, c)\n",
    "            out, hidden = self.lstm(x, hidden)\n",
    "            batch, seq_len, hid = out.size()\n",
    "            out_flat = out.contiguous().view(batch * seq_len, hid)\n",
    "            soc_flat = self.mlp(out_flat)\n",
    "            soc = soc_flat.view(batch, seq_len)\n",
    "            return soc, hidden\n",
    "    model = SOHModel().to(device)\n",
    "    # 2) init weights & optimize cuDNN for multi-layer LSTM\n",
    "    model.apply(init_weights)\n",
    "    model.lstm.flatten_parameters()\n",
    "    return model\n",
    "\n",
    "# Helper-Funktion für die Initialisierung der hidden states\n",
    "def init_hidden(model, batch_size=1, device=None):\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    h = torch.zeros(model.lstm.num_layers, batch_size, model.lstm.hidden_size, device=device)\n",
    "    c = torch.zeros_like(h)\n",
    "    return h, c\n",
    "\n",
    "# ——— Neue Seq-to-Seq-Funktion für Validierung/Test —————————————————————————\n",
    "def evaluate_seq2seq(model, df, device):\n",
    "    \"\"\"\n",
    "    Seq-to-Seq-Validation mit Chunking und TQDM.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    seq    = df[[\"Voltage[V]\", \"Current[A]\", \"Q_m\"]].values\n",
    "    labels = df[\"SOH_ZHU\"].values\n",
    "    total = len(seq)\n",
    "    n_chunks = math.ceil(total / SEQ_CHUNK_SIZE)\n",
    "    h, c = init_hidden(model, batch_size=1, device=device)\n",
    "    h, c = h.contiguous(), c.contiguous()\n",
    "    preds = []\n",
    "\n",
    "    print(\">> Seq2Seq-Validation startet\")\n",
    "    with torch.no_grad():\n",
    "        for i in range(n_chunks):\n",
    "            s = i * SEQ_CHUNK_SIZE\n",
    "            e = min(s + SEQ_CHUNK_SIZE, total)\n",
    "            chunk = torch.tensor(seq[s:e], dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            chunk = chunk.contiguous()\n",
    "            model.lstm.flatten_parameters()\n",
    "            # disable cuDNN here, um lange/sehr große Chunks zu erlauben\n",
    "            with torch.backends.cudnn.flags(enabled=False):\n",
    "                out, (h, c) = model(chunk, (h, c))\n",
    "            h, c = h.contiguous(), c.contiguous()\n",
    "            preds.extend(out.squeeze(0).cpu().numpy())\n",
    "    preds = np.array(preds)\n",
    "    gts = labels[: len(preds)]\n",
    "    return np.mean((preds - gts) ** 2)\n",
    "\n",
    "def evaluate_online(model, df, device):\n",
    "    \"\"\"Stepwise seq‐to‐seq Validation mit tqdm.\"\"\"\n",
    "    model.eval()\n",
    "    print(\">> Online-Validation startet\")\n",
    "    # initialize hidden state and result lists\n",
    "    h, c = init_hidden(model, batch_size=1, device=device)\n",
    "    preds, gts = [], []\n",
    "    with torch.no_grad():\n",
    "        for idx, (v, i, qm, y_true) in enumerate(zip(\n",
    "            df['Voltage[V]'], df['Current[A]'], df['Q_m'], df['SOH_ZHU']\n",
    "        )):\n",
    "            x = torch.tensor([[v, i, qm]], dtype=torch.float32, device=device).view(1,1,3).contiguous()\n",
    "            pred, (h, c) = model(x, (h, c))\n",
    "            preds.append(pred.item())\n",
    "            gts.append(y_true)\n",
    "    preds, gts = np.array(preds), np.array(gts)\n",
    "    return np.mean((preds - gts)**2)\n",
    "\n",
    "def evaluate_onechunk_seq2seq(model, df, device):\n",
    "    \"\"\"\n",
    "    Seq2Seq-Eval über genau einen Chunk: ganzes df als (1, N, F)-Sequenz.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    seq    = df[[\"Voltage[V]\",\"Current[A]\",\"Q_m\"]].values\n",
    "    labels = df[\"SOH_ZHU\"].values\n",
    "    h, c   = init_hidden(model, batch_size=1, device=device)\n",
    "    # ensure hidden states contiguous\n",
    "    h, c   = h.contiguous(), c.contiguous()\n",
    "    chunk  = torch.tensor(seq, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    # ensure input contiguous\n",
    "    chunk  = chunk.contiguous()\n",
    "    with torch.no_grad():\n",
    "        model.lstm.flatten_parameters()\n",
    "        # disable cuDNN hier, um sehr lange Ein-Chuck-Sequenzen zu erlauben\n",
    "        with torch.backends.cudnn.flags(enabled=False):\n",
    "            out, _ = model(chunk, (h, c))\n",
    "    preds = out.squeeze(0).cpu().numpy()\n",
    "    mse   = np.mean((preds - labels)**2)\n",
    "    return mse, preds, labels\n",
    "\n",
    "# Training Funktion mit Batch-Training und Seq2Seq-Validierung\n",
    "def train_online(\n",
    "    epochs=30, lr=1e-4, online_train=False,\n",
    "    hidden_size=32, dropout=0.2,\n",
    "    patience=5, log_csv_path=\"training_log.csv\",\n",
    "    out_dir=\"trial\" , train_data=None, df_val=None, df_test=None, feature_scaler=None):\n",
    "\n",
    "    # convert out_dir to Path so \"/\" operator works\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    log_csv_path = out_dir / log_csv_path\n",
    "\n",
    "    if train_data is None:\n",
    "        train_scaled, df_val, df_test, train_cells, val_cell, feature_scaler = load_data()\n",
    "    else:\n",
    "        train_scaled = train_data\n",
    "        # reuse globals\n",
    "        train_cells = train_cells_glob\n",
    "        val_cell    = val_cell_glob\n",
    "        # passed dfs/scaler\n",
    "        feature_scaler = feature_scaler\n",
    "    print(f\"[INFO] Train cells={train_cells}, Val/Test cell={val_cell}\")\n",
    "\n",
    "    # Input size=3 (Spannung, Strom, Q_m)\n",
    "    model = build_model(input_size=3, hidden_size=hidden_size, dropout=dropout)\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = ReduceLROnPlateau(optim, mode='min', patience=3, factor=0.5)\n",
    "    criterion = nn.MSELoss()\n",
    "    gradient_scaler = GradScaler(enabled=(device.type==\"cuda\"))\n",
    "    best_val = float('inf'); no_improve = 0\n",
    "\n",
    "    # HISTORY & LOG INITIALIZATION\n",
    "    train_rmse_history = []\n",
    "    val_rmse_history   = []\n",
    "    log_rows = []  # <-- ensure log_rows exists\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        print(f\"\\n--- Epoch {ep}/{epochs} ---\")\n",
    "        model.train()\n",
    "        total_loss, steps = 0, 0\n",
    "\n",
    "        for name, df in train_scaled.items():\n",
    "            print(f\"[Epoch {ep}] Training Cell {name}\")\n",
    "            if not online_train:\n",
    "                ds = CellDataset(df, SEQ_CHUNK_SIZE)\n",
    "                dl = DataLoader(ds, batch_size=1, shuffle=False, num_workers=4, pin_memory=True)\n",
    "                h, c = init_hidden(model, batch_size=1, device=device)\n",
    "                for x_b, y_b in dl:\n",
    "                    x_b, y_b = x_b.to(device), y_b.to(device)\n",
    "                    x_b = x_b.contiguous()  # Ensure contiguous input\n",
    "                    \n",
    "                    optim.zero_grad()\n",
    "                    \n",
    "                    # Use proper precision context\n",
    "                    with autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                        model.lstm.flatten_parameters()  # Optimize LSTM\n",
    "                        pred, (h, c) = model(x_b, (h, c))\n",
    "                        loss = criterion(pred, y_b)\n",
    "                    \n",
    "                    gradient_scaler.scale(loss).backward()\n",
    "                    gradient_scaler.unscale_(optim)\n",
    "                    clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    gradient_scaler.step(optim)\n",
    "                    gradient_scaler.update()\n",
    "                    \n",
    "                    h, c = h.detach(), c.detach()\n",
    "                    total_loss += loss.item()   \n",
    "                    steps += 1\n",
    "            else:\n",
    "                print(f\"[Epoch {ep}] Online-Training Cell {name}\")\n",
    "                h, c = init_hidden(model, batch_size=1, device=device)\n",
    "                # nur Spannung, Strom, Q_m input, SOH_ZHU als Label\n",
    "                for v, i, qm, y_true in zip(\n",
    "                    df['Voltage[V]'], df['Current[A]'], df['Q_m'], df['SOH_ZHU']\n",
    "                ):\n",
    "                    x = torch.tensor([[v, i, qm]], dtype=torch.float32, device=device).view(1,1,3).contiguous()\n",
    "                    y = torch.tensor([[y_true]], dtype=torch.float32, device=device).contiguous()\n",
    "                    optim.zero_grad()\n",
    "                    \n",
    "                    with autocast(device_type=device.type, enabled=(device.type == 'cuda')):\n",
    "                        model.lstm.flatten_parameters()\n",
    "                        pred, (h, c) = model(x, (h, c))\n",
    "                        loss = criterion(pred, y)\n",
    "                    \n",
    "                    gradient_scaler.scale(loss).backward()\n",
    "                    gradient_scaler.unscale_(optim)\n",
    "                    clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    gradient_scaler.step(optim)\n",
    "                    gradient_scaler.update()\n",
    "                    \n",
    "                    h, c = h.detach(), c.detach()\n",
    "                    total_loss += loss.item()                    \n",
    "                    steps += 1\n",
    "\n",
    "        train_rmse = math.sqrt(total_loss/steps)\n",
    "        train_rmse_history.append(train_rmse)\n",
    "        print(f\"[Epoch {ep}] train RMSE={train_rmse:.4f}\")\n",
    "\n",
    "        # nur MSE berechnen, kein Plot mehr\n",
    "        val_mse, _, _ = evaluate_onechunk_seq2seq(model, df_val, device)\n",
    "        val_rmse = math.sqrt(val_mse)\n",
    "        val_rmse_history.append(val_rmse)\n",
    "        print(f\"[Epoch {ep}] val RMSE={val_rmse:.4f}\")\n",
    "\n",
    "        # Early Stopping & Model Save\n",
    "        is_best = val_rmse < best_val\n",
    "        best_val = min(val_rmse, best_val)\n",
    "        if is_best:\n",
    "            no_improve = 0\n",
    "            # Modell speichern\n",
    "            torch.save(model.state_dict(), out_dir / \"best_model.pth\")\n",
    "            print(f\"[Epoch {ep}] Modell gespeichert: {out_dir / 'best_model.pth'}\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        # Learning Rate anpassen\n",
    "        scheduler.step(val_mse)\n",
    "\n",
    "        # Logging\n",
    "        log_rows.append({\n",
    "            \"epoch\": ep,\n",
    "            \"train_rmse\": train_rmse,\n",
    "            \"val_rmse\": val_rmse,\n",
    "            \"lr\": optim.param_groups[0]['lr'],\n",
    "            \"hidden_size\": hidden_size,\n",
    "            \"dropout\": dropout\n",
    "        })\n",
    "        df_log = pd.DataFrame(log_rows)\n",
    "        df_log.to_csv(log_csv_path, index=False)\n",
    "\n",
    "        # Frühes Stoppen\n",
    "        if no_improve >= patience:\n",
    "            print(f\"[INFO] Frühes Stoppen bei Epoche {ep}\")\n",
    "            break\n",
    "\n",
    "    # Lade das beste Modell für die finale Bewertung\n",
    "    model.load_state_dict(torch.load(out_dir / \"best_model.pth\"))\n",
    "\n",
    "    # Finale Bewertung auf Val und Test\n",
    "    val_mse, val_preds, val_labels = evaluate_onechunk_seq2seq(model, df_val, device)\n",
    "    test_mse, test_preds, test_labels = evaluate_onechunk_seq2seq(model, df_test, device)\n",
    "    val_rmse = math.sqrt(val_mse)\n",
    "    test_rmse = math.sqrt(test_mse)\n",
    "    print(f\"[INFO] Finale Bewertung -> Val RMSE: {val_rmse:.4f}, Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "    # PLOT-Verlauf zeichnen\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(range(1, len(train_rmse_history)+1), train_rmse_history, label=\"Train RMSE\")\n",
    "    plt.plot(range(1, len(val_rmse_history)+1),   val_rmse_history,   label=\"Val RMSE\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Trainingsverlauf\")\n",
    "    plt.grid()\n",
    "    plt.savefig(out_dir / \"train_val_rmse_plot.png\")\n",
    "    plt.show()\n",
    "\n",
    "    return model, feature_scaler, log_rows, df_val, df_test\n",
    "\n",
    "# Global data loading & scaling once before HPT\n",
    "train_scaled_glob, df_val_glob, df_test_glob, train_cells_glob, val_cell_glob, feature_scaler_glob = load_data()\n",
    "print(\"[INFO] Global data loaded and scaled once for HPT.\")\n",
    "\n",
    "# Optuna-Optimierung für Hyperparameter\n",
    "def objective(trial):\n",
    "    # 1) Hyperparam einlesen\n",
    "    epochs = 30\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "    hidden_size = trial.suggest_int(\"hidden_size\", 16, 128, step=16)\n",
    "    dropout = trial.suggest_uniform(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    # 2) dynamischen Ordnernamen bauen\n",
    "    num = trial.number\n",
    "    dr_str = f\"{dropout:.4f}\"\n",
    "    lr_str = f\"{lr:.0e}\"\n",
    "    folder = f\"trial_{num:02d}_hs{hidden_size}_dr{dr_str}_lr{lr_str}\"\n",
    "\n",
    "    # 3) Training\n",
    "    model, feature_scaler, log_rows, df_val, df_test = train_online(\n",
    "        epochs=epochs, lr=lr,\n",
    "        hidden_size=hidden_size, dropout=dropout,\n",
    "        patience=5, out_dir=folder,\n",
    "        train_data=train_scaled_glob,\n",
    "        df_val=df_val_glob, df_test=df_test_glob,\n",
    "        feature_scaler=feature_scaler_glob\n",
    "    )\n",
    "    # Test-MAE berechnen\n",
    "    _, test_preds, test_labels = evaluate_onechunk_seq2seq(model, df_test, device)\n",
    "    test_mae = np.mean(np.abs(test_preds - test_labels))\n",
    "    Path(folder).joinpath(\"test_mae.txt\").write_text(f\"{test_mae:.6f}\")\n",
    "    # Plot und speichern\n",
    "    plt.figure()\n",
    "    plt.plot(test_labels, label=\"True SOC\")\n",
    "    plt.plot(test_preds,  label=\"Predicted SOC\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Trial {trial.number} Test SOC Prediction\")\n",
    "    plt.savefig(Path(folder) / \"test_plot.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Ziel: minimale Validierungs-RMSE\n",
    "    min_val = min(r[\"val_rmse\"] for r in log_rows)\n",
    "    return min_val\n",
    "\n",
    "# Optuna Studienlauf\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Beste Hyperparameter\n",
    "best_params = study.best_params\n",
    "print(f\"Beste Hyperparameter: {best_params}\")\n",
    "\n",
    "## Train final model mit erneutem HPT-Split (kein Re-Scaling)\n",
    "model, feature_scaler, log_rows, df_val, df_test = train_online(\n",
    "    epochs=50, lr=best_params[\"lr\"],\n",
    "    hidden_size=best_params[\"hidden_size\"], dropout=best_params[\"dropout\"],\n",
    "    patience=10, out_dir=\"final_model\",\n",
    "    train_data=train_scaled_glob,\n",
    "    df_val=df_val_glob, df_test=df_test_glob,\n",
    "    feature_scaler=feature_scaler_glob\n",
    ")\n",
    "# Finale Bewertung ohne Neu-Scaling\n",
    "val_mse, val_preds, val_labels = evaluate_onechunk_seq2seq(model, df_val, device)\n",
    "test_mse, test_preds, test_labels = evaluate_onechunk_seq2seq(model, df_test, device)\n",
    "val_rmse = math.sqrt(val_mse)\n",
    "test_rmse = math.sqrt(test_mse)\n",
    "print(f\"[INFO] Finale Bewertung -> Val RMSE: {val_rmse:.4f}, Test RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "# Speichern des finalen Modells und Scalers\n",
    "torch.save(model.state_dict(), \"final_model.pth\")\n",
    "with open(\"feature_scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(feature_scaler, f)\n",
    "\n",
    "print(\"Training abgeschlossen und Modelle gespeichert.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
