{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8274fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tqdm import tqdm                                    # ← hinzugefügt\n",
    "\n",
    "# --- Konstante Einstellung ---\n",
    "SEQ_CHUNK_SIZE = 10000\n",
    "HIDDEN_SIZE    = 32\n",
    "MLP_HIDDEN     = 32\n",
    "MODEL_PATH     = Path(\"/home/florianr/MG_Farm/6_Scripts/BMS/Python/BMS_SOC/BMS_SOC_LSTM_stateful_1.2.4_Train/BMS_SOC_LSTM_stateful_1.2.4.7_Train_CPU/training_run_1/best_model.pth\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_cell_data(data_dir: Path):\n",
    "    # statt DataFrames returnen wir hier nur die Parquet-Pfade\n",
    "    cell_paths = {}\n",
    "    for folder in sorted(data_dir.iterdir()):\n",
    "        if folder.is_dir() and folder.name.startswith(\"MGFarm_18650_\"):\n",
    "            p = folder / \"df.parquet\"\n",
    "            if p.exists():\n",
    "                cell_paths[folder.name] = p\n",
    "            else:\n",
    "                print(f\"Warning: {p} fehlt\")\n",
    "    return cell_paths\n",
    "\n",
    "def load_data(base_path: str = \"/home/florianr/MG_Farm/5_Data/MGFarm_18650_Dataframes\"):\n",
    "    base      = Path(base_path)\n",
    "    cell_paths= load_cell_data(base)\n",
    "    feats     = [\"Voltage[V]\",\"Current[A]\",\"SOH_ZHU\",\"Q_m\"]\n",
    "    # iteratives Fitten\n",
    "    scaler = StandardScaler()\n",
    "    for name, p in cell_paths.items():\n",
    "        df_tmp = pd.read_parquet(p, columns=feats)\n",
    "        df_tmp.dropna(subset=feats, inplace=True)\n",
    "        if not df_tmp.empty:\n",
    "            scaler.partial_fit(df_tmp[feats])\n",
    "    print(\"[INFO] Scaler iterativ gefittet\")\n",
    "\n",
    "    # Validierungszellen\n",
    "    val_cells = [\"MGFarm_18650_C01\",\"MGFarm_18650_C03\",\"MGFarm_18650_C05\"]\n",
    "    df_vals   = {}\n",
    "    for name in val_cells:\n",
    "        if name not in cell_paths:\n",
    "            print(f\"Warning: {name} nicht gefunden, übersprungen\")\n",
    "            continue\n",
    "        df = pd.read_parquet(cell_paths[name])\n",
    "        df['timestamp'] = pd.to_datetime(df['Absolute_Time[yyyy-mm-dd hh:mm:ss]'])\n",
    "        df[feats]      = scaler.transform(df[feats])\n",
    "        df_vals[name]  = df\n",
    "\n",
    "    # für Test brauchen wir Trainingsdaten nicht\n",
    "    return {}, df_vals, [], val_cells, scaler\n",
    "\n",
    "# Angepasstes Dataset für ganze Zellen\n",
    "class CellDataset(Dataset):\n",
    "    def __init__(self, df, sequence_length=SEQ_CHUNK_SIZE):\n",
    "        \"\"\"Dataset für eine ganze Zelle, aufgeteilt in Sequenz-Chunks\"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "        self.data   = df[[\"Voltage[V]\", \"Current[A]\", \"SOH_ZHU\", \"Q_m\"]].values\n",
    "        self.labels = df[\"SOC_ZHU\"].values\n",
    "        self.n_batches = max(1, len(self.data) // self.sequence_length)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_batches  # Anzahl der Batches\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * self.sequence_length\n",
    "        end = min(start + self.sequence_length, len(self.data))\n",
    "        x = torch.from_numpy(self.data[start:end]).float()\n",
    "        y = torch.from_numpy(self.labels[start:end]).float()\n",
    "        return x, y\n",
    "\n",
    "# Weight-initialization helper\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.LSTM):\n",
    "        for name, p in m.named_parameters():\n",
    "            if 'weight_ih' in name or 'weight_hh' in name:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.constant_(p, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# Modell: LSTM + Dropout + MLP-Head (verwendet globale HIDDEN_SIZE und MLP_HIDDEN)\n",
    "def build_model(input_size=4, num_layers=1, dropout=0.1):\n",
    "    class SOCModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            # LSTM ohne Dropout (voller Informationsfluss)\n",
    "            self.lstm = nn.LSTM(input_size, HIDDEN_SIZE, num_layers,\n",
    "                                batch_first=True, dropout=0.0)\n",
    "            # hidden_size bestimmt die Dim. der LSTM-Ausgabe\n",
    "            # mlp_hidden ist die Größe der verborgenen MLP-Schicht\n",
    "            # deeper MLP-Head\n",
    "            self.mlp = nn.Sequential(\n",
    "                nn.Linear(HIDDEN_SIZE, MLP_HIDDEN),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(MLP_HIDDEN, MLP_HIDDEN),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout * 0.5),\n",
    "                nn.Linear(MLP_HIDDEN, 1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        def forward(self, x, hidden):\n",
    "            self.lstm.flatten_parameters()       # cuDNN-ready\n",
    "            x = x.contiguous()                   # ensure input contiguous\n",
    "            # make hidden states contiguous\n",
    "            h, c = hidden\n",
    "            h, c = h.contiguous(), c.contiguous()\n",
    "            hidden = (h, c)\n",
    "            out, hidden = self.lstm(x, hidden)\n",
    "            batch, seq_len, hid = out.size()\n",
    "            out_flat = out.contiguous().view(batch * seq_len, hid)\n",
    "            soc_flat = self.mlp(out_flat)\n",
    "            soc = soc_flat.view(batch, seq_len)\n",
    "            return soc, hidden\n",
    "    model = SOCModel().to(device)\n",
    "    # 2) init weights & optimize cuDNN for multi-layer LSTM\n",
    "    model.apply(init_weights)\n",
    "    model.lstm.flatten_parameters()\n",
    "    return model\n",
    "\n",
    "# Helper-Funktion für die Initialisierung der hidden states\n",
    "def init_hidden(model, batch_size=1, device=None):\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    h = torch.zeros(model.lstm.num_layers, batch_size, model.lstm.hidden_size, device=device)\n",
    "    c = torch.zeros_like(h)\n",
    "    return h, c\n",
    "\n",
    "def evaluate_onechunk_seq2seq(model, df, device):\n",
    "    \"\"\"\n",
    "    Seq2Seq-Eval über genau einen Chunk: ganzes df als (1, N, F)-Sequenz.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    seq    = df[[\"Voltage[V]\",\"Current[A]\",\"SOH_ZHU\",\"Q_m\"]].values\n",
    "    labels = df[\"SOC_ZHU\"].values\n",
    "    total = len(seq)\n",
    "    n_chunks = math.ceil(total / SEQ_CHUNK_SIZE)\n",
    "    h, c   = init_hidden(model, batch_size=1, device=device)\n",
    "    h, c   = h.contiguous(), c.contiguous()\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(n_chunks), desc=\"Test chunks\"):  # ← tqdm für Fortschritt\n",
    "            s = i * SEQ_CHUNK_SIZE\n",
    "            e = min(s + SEQ_CHUNK_SIZE, total)\n",
    "            chunk = torch.tensor(seq[s:e], dtype=torch.float32, device=device).unsqueeze(0).contiguous()\n",
    "            with torch.backends.cudnn.flags(enabled=False):\n",
    "                out, (h, c) = model(chunk, (h, c))\n",
    "            h, c = h.contiguous(), c.contiguous()\n",
    "            preds.extend(out.squeeze(0).cpu().numpy())\n",
    "\n",
    "    preds = np.array(preds)\n",
    "    gts = labels[: len(preds)]\n",
    "    return np.mean((preds - gts)**2), preds, gts\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Daten laden\n",
    "    _, df_vals, _, _, _ = load_data()\n",
    "\n",
    "    # Test nur Zelle C05|ersten 10%\n",
    "    name = \"MGFarm_18650_C05\"\n",
    "    df_full = df_vals[name]\n",
    "    n = int(len(df_full) * 0.1)\n",
    "    df_test = df_full.iloc[:n].reset_index(drop=True)\n",
    "\n",
    "    # Modell laden (weights_only=True unterdrückt FutureWarning)\n",
    "    model = build_model()\n",
    "    model.load_state_dict(\n",
    "        torch.load(MODEL_PATH, map_location=device, weights_only=True)  # ← weights_only=True\n",
    "    )\n",
    "    model.to(device).eval()\n",
    "\n",
    "    out_dir = Path(\"test_run\"); out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Evaluation & Plot\n",
    "    mse, preds, labels = evaluate_onechunk_seq2seq(model, df_test, device)\n",
    "    mae = np.mean(np.abs(preds - labels))\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(range(len(labels)), labels, label=\"true\")\n",
    "    plt.plot(range(len(preds)), preds, '--', label=f\"pred (MAE={mae:.4f})\")\n",
    "    plt.xlabel(\"Timestep\"); plt.ylabel(\"SOC\")\n",
    "    plt.title(f\"Test – {name} (10% Daten, MSE={mse:.4f})\")\n",
    "    plt.legend(); plt.grid()\n",
    "    plt.savefig(out_dir/f\"test_{name}.png\")\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
