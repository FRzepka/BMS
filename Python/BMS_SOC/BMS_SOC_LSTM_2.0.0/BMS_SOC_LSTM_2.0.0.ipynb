{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BMS_SOC_LSTM_1.2.0\n",
    "- nicht mehr autoregressiv\n",
    "- test funktioniert noch nicht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Falls vorhanden:\n",
    "from pytorch_forecasting.models.nn.rnn import LSTM as ForecastingLSTM\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def load_cell_data(data_dir: Path):\n",
    "    \"\"\"\n",
    "    Lädt df.parquet aus dem Unterordner 'MGFarm_18650_C01'.\n",
    "    \"\"\"\n",
    "    dataframes = {}\n",
    "    folder = data_dir / \"MGFarm_18650_C01\"\n",
    "    if folder.exists() and folder.is_dir():\n",
    "        df_path = folder / 'df.parquet'\n",
    "        if df_path.exists():\n",
    "            df = pd.read_parquet(df_path)\n",
    "            dataframes[\"C01\"] = df\n",
    "            print(f\"Loaded {folder.name}\")\n",
    "        else:\n",
    "            print(f\"Warning: No df.parquet found in {folder.name}\")\n",
    "    else:\n",
    "        print(\"Warning: Folder MGFarm_18650_C01 not found\")\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "def main():\n",
    "    ########################################################################\n",
    "    # 1) Daten laden und vorbereiten\n",
    "    ########################################################################\n",
    "    data_dir = Path('/home/florianr/MG_Farm/5_Data/MGFarm_18650_Dataframes')\n",
    "    cell_data = load_cell_data(data_dir)\n",
    "\n",
    "    # Nimm die erste gefundene Zelle\n",
    "    cell_keys = sorted(cell_data.keys())[:1]\n",
    "    if len(cell_keys) < 1:\n",
    "        raise ValueError(\"Keine Zelle gefunden; bitte prüfen.\")\n",
    "\n",
    "    cell_name = cell_keys[0]\n",
    "    df_full = cell_data[cell_name]\n",
    "\n",
    "    # Reduziere auf 25% der Daten (kannst du anpassen)\n",
    "    sample_size = int(len(df_full) * 0.25)\n",
    "    df_small = df_full.head(sample_size).copy()\n",
    "\n",
    "    print(f\"Gesamtdaten: {len(df_full)}, wir nehmen 25% = {sample_size} Zeilen.\")\n",
    "\n",
    "    # Timestamp anlegen (nur für Plot)\n",
    "    df_small['timestamp'] = pd.to_datetime(df_small['Absolute_Time[yyyy-mm-dd hh:mm:ss]'])\n",
    "\n",
    "    # Zeitbasierter Split: 40% Train, 40% Val, 20% Test\n",
    "    len_small = len(df_small)\n",
    "    train_end = int(len_small * 0.4)\n",
    "    val_end   = int(len_small * 0.8)\n",
    "\n",
    "    df_train = df_small.iloc[:train_end]\n",
    "    df_val   = df_small.iloc[train_end:val_end]\n",
    "    df_test  = df_small.iloc[val_end:]\n",
    "\n",
    "    print(f\"Train: {len(df_train)}  |  Val: {len(df_val)}  |  Test: {len(df_test)}\")\n",
    "\n",
    "    ########################################################################\n",
    "    # 2) Skalierung von Voltage & Current (NICHT SOC)\n",
    "    ########################################################################\n",
    "    scaler = StandardScaler()\n",
    "    features_to_scale = ['Voltage[V]', 'Current[A]']\n",
    "\n",
    "    # Fit nur auf Training\n",
    "    scaler.fit(df_train[features_to_scale])\n",
    "\n",
    "    df_train_scaled = df_train.copy()\n",
    "    df_val_scaled   = df_val.copy()\n",
    "    df_test_scaled  = df_test.copy()\n",
    "\n",
    "    df_train_scaled[features_to_scale] = scaler.transform(df_train_scaled[features_to_scale])\n",
    "    df_val_scaled[features_to_scale]   = scaler.transform(df_val_scaled[features_to_scale])\n",
    "    df_test_scaled[features_to_scale]  = scaler.transform(df_test_scaled[features_to_scale])\n",
    "\n",
    "    ########################################################################\n",
    "    # 3) Dataset-Klasse (seq2one, NICHT autoregressiv)\n",
    "    ########################################################################\n",
    "    class SequenceDataset(Dataset):\n",
    "        \"\"\"\n",
    "        - X[t] = [Voltage, Current] für t..t+seq_len-1\n",
    "        - y[t] = SOC an t+seq_len\n",
    "        \"\"\"\n",
    "        def __init__(self, df, seq_len=60):\n",
    "            self.seq_len = seq_len\n",
    "            # Nur Voltage und Current als Features\n",
    "            self.features = df[[\"Voltage[V]\", \"Current[A]\"]].values\n",
    "            # SOC als Label\n",
    "            self.labels = df[\"SOC_ZHU\"].values\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels) - self.seq_len\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            x_seq = self.features[idx : idx + self.seq_len]   # shape (seq_len, 2)\n",
    "            y_val = self.labels[idx + self.seq_len]           # SOC-Wert\n",
    "            return torch.tensor(x_seq, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "    # Erstellen der Datasets\n",
    "    seq_length = 60\n",
    "    train_dataset = SequenceDataset(df_train_scaled, seq_len=seq_length)\n",
    "    val_dataset   = SequenceDataset(df_val_scaled,   seq_len=seq_length)\n",
    "    test_dataset  = SequenceDataset(df_test_scaled,  seq_len=seq_length)\n",
    "\n",
    "    # DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=50000, shuffle=True, drop_last=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=50000, shuffle=False, drop_last=True)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=50000, shuffle=False, drop_last=True)\n",
    "\n",
    "    ########################################################################\n",
    "    # 4) LSTM-Modell + (optional) DirectionLoss\n",
    "    ########################################################################\n",
    "    # Falls du pytorch_forecasting nicht hast, kannst du stattdessen nn.LSTM nehmen.\n",
    "    class LSTMSOCModel(nn.Module):\n",
    "        def __init__(self, input_size=2, hidden_size=32, num_layers=2, batch_first=True):\n",
    "            super().__init__()\n",
    "            self.lstm = ForecastingLSTM(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=batch_first\n",
    "            )\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            lstm_out, _ = self.lstm(x)\n",
    "            last_out = lstm_out[:, -1, :]\n",
    "            soc_pred = self.fc(last_out)\n",
    "            return soc_pred.squeeze(-1)\n",
    "\n",
    "    class DirectionLoss(nn.Module):\n",
    "        \"\"\"\n",
    "        MSE + Strafterm für \"falsche\" Drehrichtung.\n",
    "        \"\"\"\n",
    "        def __init__(self, alpha=0.1):\n",
    "            super().__init__()\n",
    "            self.mse = nn.MSELoss()\n",
    "            self.alpha = alpha\n",
    "\n",
    "        def forward(self, y_pred, y_true, x_seq):\n",
    "            # 1) Standard MSE\n",
    "            loss_mse = self.mse(y_pred, y_true)\n",
    "\n",
    "            # 2) Strafterm (optional)\n",
    "            current  = x_seq[:, -1, 1]  # Letzter Current aus dem Fenster\n",
    "            soc_last = y_true           # Wir tun so, als wäre der \"letzte SOC\" der ground truth\n",
    "\n",
    "            penalty_up   = torch.clamp(soc_last - y_pred, min=0.0) * (current > 0).float()\n",
    "            penalty_down = torch.clamp(y_pred - soc_last, min=0.0) * (current < 0).float()\n",
    "            penalty_direction = (penalty_up + penalty_down).mean()\n",
    "\n",
    "            total_loss = loss_mse + self.alpha * penalty_direction\n",
    "            return total_loss\n",
    "\n",
    "    model = LSTMSOCModel(input_size=2, hidden_size=32, num_layers=2, batch_first=True)\n",
    "\n",
    "    # Standard-Loss oder DirectionLoss\n",
    "    criterion = DirectionLoss(alpha=0.1)\n",
    "    # criterion = nn.MSELoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "    ########################################################################\n",
    "    # 5) Training mit Validation + Early Stopping\n",
    "    ########################################################################\n",
    "    # Plot: Datenaufteilung\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(df_small['timestamp'], df_small['SOC_ZHU'], 'k-', label='SOC (alle Daten)')\n",
    "    plt.axvspan(df_train['timestamp'].iloc[0],\n",
    "                df_train['timestamp'].iloc[-1],\n",
    "                color='green', alpha=0.3, label='Training')\n",
    "    plt.axvspan(df_val['timestamp'].iloc[0],\n",
    "                df_val['timestamp'].iloc[-1],\n",
    "                color='orange', alpha=0.3, label='Validation')\n",
    "    plt.axvspan(df_test['timestamp'].iloc[0],\n",
    "                df_test['timestamp'].iloc[-1],\n",
    "                color='red', alpha=0.3, label='Test')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('SOC (ZHU)')\n",
    "    plt.title('Datenaufteilung')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    epochs = 50\n",
    "    patience = 10\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    # Gradient Accumulation\n",
    "    accumulation_steps = 10\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            y_pred = model(x_batch)\n",
    "            loss = criterion(y_pred, y_batch, x_batch) / accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            train_losses.append(loss.item() * accumulation_steps)\n",
    "\n",
    "        mean_train_loss = np.mean(train_losses)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        all_y_val = []\n",
    "        all_y_pred = []\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in val_loader:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                y_pred_val = model(x_val)\n",
    "                v_loss = criterion(y_pred_val, y_val, x_val)\n",
    "                val_losses.append(v_loss.item())\n",
    "                # ersten Batch zum Plot\n",
    "                if not all_y_val:\n",
    "                    all_y_val.append(y_val.cpu().numpy())\n",
    "                    all_y_pred.append(y_pred_val.cpu().numpy())\n",
    "\n",
    "        mean_val_loss = np.mean(val_losses)\n",
    "\n",
    "        # Early Stopping\n",
    "        if mean_val_loss < best_val_loss:\n",
    "            best_val_loss = mean_val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} because val_loss not improved.\")\n",
    "                break\n",
    "\n",
    "        print(f\"Epoch {epoch:03d}/{epochs}, \"\n",
    "              f\"Train Loss: {mean_train_loss:.6f}, \"\n",
    "              f\"Val Loss: {mean_val_loss:.6f}, \"\n",
    "              f\"NoImprove: {epochs_no_improve}\")\n",
    "\n",
    "        # Plot Validation Predictions (nur erster Batch)\n",
    "        y_val_example = all_y_val[0].flatten()\n",
    "        y_pred_example = all_y_pred[0].flatten()\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.plot(y_val_example, label='Ground Truth')\n",
    "        plt.plot(y_pred_example, label='Predicted')\n",
    "        plt.title(f\"Validation Predictions at Epoch {epoch}\")\n",
    "        plt.xlabel(\"Sample Index\")\n",
    "        plt.ylabel(\"SOC\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show(block=False)\n",
    "        plt.pause(1)\n",
    "        plt.close()\n",
    "\n",
    "    # Bestes Modell wiederherstellen\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"\\nBest model reloaded with val_loss = {best_val_loss:.6f}\")\n",
    "\n",
    "    # Speichern\n",
    "    models_dir = Path(\"models\")\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    \n",
    "    model_path = models_dir / \"best_lstm_soc_model.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Bestes Modell gespeichert unter: {model_path}\")\n",
    "\n",
    "    ########################################################################\n",
    "    # 6) Test-Vorhersage (NICHT autoregressiv)\n",
    "    ########################################################################\n",
    "    # Lade dasselbe Modell nochmal\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    test_predictions = []\n",
    "    test_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_test, y_test in test_loader:\n",
    "            x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "            y_pred_test = model(x_test)\n",
    "            test_predictions.append(y_pred_test.cpu().numpy())\n",
    "            test_targets.append(y_test.cpu().numpy())\n",
    "\n",
    "    test_predictions = np.concatenate(test_predictions)\n",
    "    test_targets = np.concatenate(test_targets)\n",
    "\n",
    "    # Zeitstempel anpassen\n",
    "    time_test = df_test['timestamp'].values[seq_length:seq_length + len(test_targets)]\n",
    "\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(time_test, test_targets, label=\"Ground Truth SOC\", linestyle='-')\n",
    "    plt.plot(time_test, test_predictions, label=\"Predicted SOC (LSTM)\", linestyle='--')\n",
    "    plt.title(f\"Standard LSTM-Vorhersage - Test (Zelle: {cell_name})\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"SOC (ZHU)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_file = models_dir / \"prediction_test.png\"\n",
    "    plt.savefig(plot_file)\n",
    "    plt.show()\n",
    "    print(f\"Test-Plot gespeichert unter: {plot_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BMS_SOC_LSTM_2.0.0\n",
    "- nicht mehr autoregressiv\n",
    "- test funktioniert noch nicht wird hier getestet !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Falls vorhanden:\n",
    "from pytorch_forecasting.models.nn.rnn import LSTM as ForecastingLSTM\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def load_cell_data(data_dir: Path):\n",
    "    \"\"\"\n",
    "    Lädt df.parquet aus dem Unterordner 'MGFarm_18650_C01'.\n",
    "    \"\"\"\n",
    "    dataframes = {}\n",
    "    folder = data_dir / \"MGFarm_18650_C01\"\n",
    "    if folder.exists() and folder.is_dir():\n",
    "        df_path = folder / 'df.parquet'\n",
    "        if df_path.exists():\n",
    "            df = pd.read_parquet(df_path)\n",
    "            dataframes[\"C01\"] = df\n",
    "            print(f\"Loaded {folder.name}\")\n",
    "        else:\n",
    "            print(f\"Warning: No df.parquet found in {folder.name}\")\n",
    "    else:\n",
    "        print(\"Warning: Folder MGFarm_18650_C01 not found\")\n",
    "    return dataframes\n",
    "\n",
    "\n",
    "def main():\n",
    "    ########################################################################\n",
    "    # 1) Daten laden und vorbereiten\n",
    "    ########################################################################\n",
    "    data_dir = Path('/home/florianr/MG_Farm/5_Data/MGFarm_18650_Dataframes')\n",
    "    cell_data = load_cell_data(data_dir)\n",
    "\n",
    "    # Nimm die erste gefundene Zelle\n",
    "    cell_keys = sorted(cell_data.keys())[:1]\n",
    "    if len(cell_keys) < 1:\n",
    "        raise ValueError(\"Keine Zelle gefunden; bitte prüfen.\")\n",
    "\n",
    "    cell_name = cell_keys[0]\n",
    "    df_full = cell_data[cell_name]\n",
    "\n",
    "    # Reduziere auf 25% der Daten (kannst du anpassen)\n",
    "    sample_size = int(len(df_full) * 0.25)\n",
    "    df_small = df_full.head(sample_size).copy()\n",
    "\n",
    "    print(f\"Gesamtdaten: {len(df_full)}, wir nehmen 25% = {sample_size} Zeilen.\")\n",
    "\n",
    "    # Timestamp anlegen (nur für Plot)\n",
    "    df_small['timestamp'] = pd.to_datetime(df_small['Absolute_Time[yyyy-mm-dd hh:mm:ss]'])\n",
    "\n",
    "    # Zeitbasierter Split: 40% Train, 40% Val, 20% Test\n",
    "    len_small = len(df_small)\n",
    "    train_end = int(len_small * 0.4)\n",
    "    val_end   = int(len_small * 0.8)\n",
    "\n",
    "    df_train = df_small.iloc[:train_end]\n",
    "    df_val   = df_small.iloc[train_end:val_end]\n",
    "    df_test  = df_small.iloc[val_end:]\n",
    "\n",
    "    print(f\"Train: {len(df_train)}  |  Val: {len(df_val)}  |  Test: {len(df_test)}\")\n",
    "\n",
    "    ########################################################################\n",
    "    # 2) Skalierung von Voltage & Current (NICHT SOC)\n",
    "    ########################################################################\n",
    "    scaler = StandardScaler()\n",
    "    features_to_scale = ['Voltage[V]', 'Current[A]']\n",
    "\n",
    "    # Fit nur auf Training\n",
    "    scaler.fit(df_train[features_to_scale])\n",
    "\n",
    "    df_train_scaled = df_train.copy()\n",
    "    df_val_scaled   = df_val.copy()\n",
    "    df_test_scaled  = df_test.copy()\n",
    "\n",
    "    df_train_scaled[features_to_scale] = scaler.transform(df_train_scaled[features_to_scale])\n",
    "    df_val_scaled[features_to_scale]   = scaler.transform(df_val_scaled[features_to_scale])\n",
    "    df_test_scaled[features_to_scale]  = scaler.transform(df_test_scaled[features_to_scale])\n",
    "\n",
    "    ########################################################################\n",
    "    # 3) Dataset-Klasse (seq2one, NICHT autoregressiv)\n",
    "    ########################################################################\n",
    "    class SequenceDataset(Dataset):\n",
    "        \"\"\"\n",
    "        - X[t] = [Voltage, Current] für t..t+seq_len-1\n",
    "        - y[t] = SOC an t+seq_len\n",
    "        \"\"\"\n",
    "        def __init__(self, df, seq_len=60):\n",
    "            self.seq_len = seq_len\n",
    "            # Nur Voltage und Current als Features\n",
    "            self.features = df[[\"Voltage[V]\", \"Current[A]\"]].values\n",
    "            # SOC als Label\n",
    "            self.labels = df[\"SOC_ZHU\"].values\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.labels) - self.seq_len\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            x_seq = self.features[idx : idx + self.seq_len]   # shape (seq_len, 2)\n",
    "            y_val = self.labels[idx + self.seq_len]           # SOC-Wert\n",
    "            return torch.tensor(x_seq, dtype=torch.float32), torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "    # Erstellen der Datasets\n",
    "    seq_length = 60\n",
    "    train_dataset = SequenceDataset(df_train_scaled, seq_len=seq_length)\n",
    "    val_dataset   = SequenceDataset(df_val_scaled,   seq_len=seq_length)\n",
    "    test_dataset  = SequenceDataset(df_test_scaled,  seq_len=seq_length)\n",
    "\n",
    "    # DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=50000, shuffle=True, drop_last=True)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=50000, shuffle=False, drop_last=True)\n",
    "    test_loader  = DataLoader(test_dataset,  batch_size=50000, shuffle=False, drop_last=True)\n",
    "\n",
    "    ########################################################################\n",
    "    # 4) LSTM-Modell + (optional) DirectionLoss\n",
    "    ########################################################################\n",
    "    # Falls du pytorch_forecasting nicht hast, kannst du stattdessen nn.LSTM nehmen.\n",
    "    class LSTMSOCModel(nn.Module):\n",
    "        def __init__(self, input_size=2, hidden_size=32, num_layers=2, batch_first=True):\n",
    "            super().__init__()\n",
    "            self.lstm = ForecastingLSTM(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=batch_first\n",
    "            )\n",
    "            self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        def forward(self, x):\n",
    "            lstm_out, _ = self.lstm(x)\n",
    "            last_out = lstm_out[:, -1, :]\n",
    "            soc_pred = self.fc(last_out)\n",
    "            return soc_pred.squeeze(-1)\n",
    "\n",
    "    class DirectionLoss(nn.Module):\n",
    "        \"\"\"\n",
    "        MSE + Strafterm für \"falsche\" Drehrichtung.\n",
    "        \"\"\"\n",
    "        def __init__(self, alpha=0.1):\n",
    "            super().__init__()\n",
    "            self.mse = nn.MSELoss()\n",
    "            self.alpha = alpha\n",
    "\n",
    "        def forward(self, y_pred, y_true, x_seq):\n",
    "            # 1) Standard MSE\n",
    "            loss_mse = self.mse(y_pred, y_true)\n",
    "\n",
    "            # 2) Strafterm (optional)\n",
    "            current  = x_seq[:, -1, 1]  # Letzter Current aus dem Fenster\n",
    "            soc_last = y_true           # Wir tun so, als wäre der \"letzte SOC\" der ground truth\n",
    "\n",
    "            penalty_up   = torch.clamp(soc_last - y_pred, min=0.0) * (current > 0).float()\n",
    "            penalty_down = torch.clamp(y_pred - soc_last, min=0.0) * (current < 0).float()\n",
    "            penalty_direction = (penalty_up + penalty_down).mean()\n",
    "\n",
    "            total_loss = loss_mse + self.alpha * penalty_direction\n",
    "            return total_loss\n",
    "\n",
    "    model = LSTMSOCModel(input_size=2, hidden_size=32, num_layers=2, batch_first=True)\n",
    "\n",
    "    # Standard-Loss oder DirectionLoss\n",
    "    criterion = DirectionLoss(alpha=0.1)\n",
    "    # criterion = nn.MSELoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "    ########################################################################\n",
    "    # 5) Training mit Validation + Early Stopping\n",
    "    ########################################################################\n",
    "    # Plot: Datenaufteilung\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(df_small['timestamp'], df_small['SOC_ZHU'], 'k-', label='SOC (alle Daten)')\n",
    "    plt.axvspan(df_train['timestamp'].iloc[0],\n",
    "                df_train['timestamp'].iloc[-1],\n",
    "                color='green', alpha=0.3, label='Training')\n",
    "    plt.axvspan(df_val['timestamp'].iloc[0],\n",
    "                df_val['timestamp'].iloc[-1],\n",
    "                color='orange', alpha=0.3, label='Validation')\n",
    "    plt.axvspan(df_test['timestamp'].iloc[0],\n",
    "                df_test['timestamp'].iloc[-1],\n",
    "                color='red', alpha=0.3, label='Test')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('SOC (ZHU)')\n",
    "    plt.title('Datenaufteilung')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    epochs = 50\n",
    "    patience = 10\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    # Gradient Accumulation\n",
    "    accumulation_steps = 10\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            y_pred = model(x_batch)\n",
    "            loss = criterion(y_pred, y_batch, x_batch) / accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            train_losses.append(loss.item() * accumulation_steps)\n",
    "\n",
    "        mean_train_loss = np.mean(train_losses)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        all_y_val = []\n",
    "        all_y_pred = []\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in val_loader:\n",
    "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
    "                y_pred_val = model(x_val)\n",
    "                v_loss = criterion(y_pred_val, y_val, x_val)\n",
    "                val_losses.append(v_loss.item())\n",
    "                # ersten Batch zum Plot\n",
    "                if not all_y_val:\n",
    "                    all_y_val.append(y_val.cpu().numpy())\n",
    "                    all_y_pred.append(y_pred_val.cpu().numpy())\n",
    "\n",
    "        mean_val_loss = np.mean(val_losses)\n",
    "\n",
    "        # Early Stopping\n",
    "        if mean_val_loss < best_val_loss:\n",
    "            best_val_loss = mean_val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch} because val_loss not improved.\")\n",
    "                break\n",
    "\n",
    "        print(f\"Epoch {epoch:03d}/{epochs}, \"\n",
    "              f\"Train Loss: {mean_train_loss:.6f}, \"\n",
    "              f\"Val Loss: {mean_val_loss:.6f}, \"\n",
    "              f\"NoImprove: {epochs_no_improve}\")\n",
    "\n",
    "        # Plot Validation Predictions (nur erster Batch)\n",
    "        y_val_example = all_y_val[0].flatten()\n",
    "        y_pred_example = all_y_pred[0].flatten()\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.plot(y_val_example, label='Ground Truth')\n",
    "        plt.plot(y_pred_example, label='Predicted')\n",
    "        plt.title(f\"Validation Predictions at Epoch {epoch}\")\n",
    "        plt.xlabel(\"Sample Index\")\n",
    "        plt.ylabel(\"SOC\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show(block=False)\n",
    "        plt.pause(1)\n",
    "        plt.close()\n",
    "\n",
    "    # Bestes Modell wiederherstellen\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"\\nBest model reloaded with val_loss = {best_val_loss:.6f}\")\n",
    "\n",
    "    # Speichern\n",
    "    models_dir = Path(\"models\")\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    \n",
    "    model_path = models_dir / \"best_lstm_soc_model.pth\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Bestes Modell gespeichert unter: {model_path}\")\n",
    "\n",
    "    ########################################################################\n",
    "    # 6) Test-Vorhersage (NICHT autoregressiv)\n",
    "    ########################################################################\n",
    "    # Lade dasselbe Modell nochmal\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    test_predictions = []\n",
    "    test_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_test, y_test in test_loader:\n",
    "            x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "            y_pred_test = model(x_test)\n",
    "            test_predictions.append(y_pred_test.cpu().numpy())\n",
    "            test_targets.append(y_test.cpu().numpy())\n",
    "\n",
    "    test_predictions = np.concatenate(test_predictions)\n",
    "    test_targets = np.concatenate(test_targets)\n",
    "\n",
    "    # Zeitstempel anpassen\n",
    "    time_test = df_test['timestamp'].values[seq_length:seq_length + len(test_targets)]\n",
    "\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(time_test, test_targets, label=\"Ground Truth SOC\", linestyle='-')\n",
    "    plt.plot(time_test, test_predictions, label=\"Predicted SOC (LSTM)\", linestyle='--')\n",
    "    plt.title(f\"Standard LSTM-Vorhersage - Test (Zelle: {cell_name})\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"SOC (ZHU)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plot_file = models_dir / \"prediction_test.png\"\n",
    "    plt.savefig(plot_file)\n",
    "    plt.show()\n",
    "    print(f\"Test-Plot gespeichert unter: {plot_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
