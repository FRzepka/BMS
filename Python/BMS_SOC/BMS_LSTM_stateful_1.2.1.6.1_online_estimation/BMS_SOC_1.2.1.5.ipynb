{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbee444c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CUDA device: NVIDIA A30\n",
      "Training auf Zellen: ['MGFarm_18650_C01', 'MGFarm_18650_C03']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_65849/890133781.py:134: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # remove deprecated 'device' kwarg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/20 ===\n",
      "--> MGFarm_18650_C01, Schritte: 17166946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MGFarm_18650_C01 Ep1:   0%|          | 0/134117 [00:00<?, ?it/s]/tmp/ipykernel_65849/890133781.py:157: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "MGFarm_18650_C01 Ep1:  53%|█████▎    | 71716/134117 [08:07<07:04, 147.02it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 210\u001b[0m\n\u001b[1;32m    207\u001b[0m         plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mZoom \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m); plt\u001b[38;5;241m.\u001b[39mtight_layout(); plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzoom_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_online_plot.png\u001b[39m\u001b[38;5;124m\"\u001b[39m); plt\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 210\u001b[0m     \u001b[43mtrain_online\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m     test_online()\n",
      "Cell \u001b[0;32mIn[4], line 160\u001b[0m, in \u001b[0;36mtrain_online\u001b[0;34m(epochs, lr, seq_len, batch_size)\u001b[0m\n\u001b[1;32m    158\u001b[0m     pred, _ \u001b[38;5;241m=\u001b[39m model(x_b, (h0, c0))\n\u001b[1;32m    159\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(pred, y_b)\n\u001b[0;32m--> 160\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m scaler\u001b[38;5;241m.\u001b[39munscale_(optim)\n\u001b[1;32m    162\u001b[0m clip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml1/lib/python3.9/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/ml1/lib/python3.9/site-packages/torch/autograd/__init__.py:340\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    331\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    332\u001b[0m     (inputs,)\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[1;32m    337\u001b[0m )\n\u001b[1;32m    339\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 340\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/anaconda3/envs/ml1/lib/python3.9/site-packages/torch/autograd/__init__.py:220\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[1;32m    219\u001b[0m         new_grads\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 220\u001b[0m             \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m         )\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm  # besser in reinen Terminalscreens\n",
    "\n",
    "# Gerät auswählen und cuDNN optimieren\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == 'cuda':\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Datenlade-Funktion\n",
    "def load_cell_data(data_dir: Path):\n",
    "    dataframes = {}\n",
    "    for folder in sorted(data_dir.iterdir()):\n",
    "        if folder.is_dir() and folder.name.startswith(\"MGFarm_18650_\"):\n",
    "            dfp = folder / \"df.parquet\"\n",
    "            if dfp.exists():\n",
    "                dataframes[folder.name] = pd.read_parquet(dfp)\n",
    "            else:\n",
    "                print(f\"Warning: {dfp} fehlt\")\n",
    "    return dataframes\n",
    "\n",
    "# Daten vorbereiten\n",
    "def load_data(base_path: str = \"/home/florianr/MG_Farm/5_Data/MGFarm_18650_Dataframes\"):\n",
    "    base = Path(base_path)\n",
    "    cells = load_cell_data(base)\n",
    "    names = sorted(cells.keys())\n",
    "    train_cells, val_cell = names[:2], names[2]\n",
    "\n",
    "    feats = [\"Voltage[V]\", \"Current[A]\"]\n",
    "    # Trainingsdaten laden und Timestamp\n",
    "    train_dfs = {}\n",
    "    for name in train_cells:\n",
    "        df = cells[name].copy()\n",
    "        df['timestamp'] = pd.to_datetime(df['Absolute_Time[yyyy-mm-dd hh:mm:ss]'])\n",
    "        train_dfs[name] = df\n",
    "\n",
    "    # Skalar fitten\n",
    "    df_all_train = pd.concat(train_dfs.values(), ignore_index=True)\n",
    "    scaler = StandardScaler().fit(df_all_train[feats])\n",
    "\n",
    "    # Skalierte Trainingsdaten\n",
    "    train_scaled = {}\n",
    "    for name, df in train_dfs.items():\n",
    "        df2 = df.copy()\n",
    "        df2[feats] = scaler.transform(df2[feats])\n",
    "        train_scaled[name] = df2\n",
    "\n",
    "    # Validierung/Test der dritten Zelle\n",
    "    df3 = cells[val_cell].copy()\n",
    "    df3['timestamp'] = pd.to_datetime(df3['Absolute_Time[yyyy-mm-dd hh:mm:ss]'])\n",
    "    L = len(df3)\n",
    "    i1, i2 = int(L*0.4), int(L*0.8)\n",
    "    df_val = df3.iloc[:i1].copy()\n",
    "    df_test = df3.iloc[i2:].copy()\n",
    "    df_val[feats] = scaler.transform(df_val[feats])\n",
    "    df_test[feats] = scaler.transform(df_test[feats])\n",
    "\n",
    "    return train_scaled, df_val, df_test, train_cells, val_cell\n",
    "\n",
    "# Windowed Dataset für schnelles Training\n",
    "class WindowedDataset(Dataset):\n",
    "    def __init__(self, df, seq_len=100):  # vorher 50\n",
    "        self.data = df[[\"Voltage[V]\", \"Current[A]\"]].values\n",
    "        self.labels = df[\"SOC_ZHU\"].values\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.from_numpy(self.data[idx:idx+self.seq_len]).float() # geht er von 0 bis 100 durch und übergibt dann h und c an 1 bis 101 weiter ? idx:idx müsste die sequenzen durchzählen idx*seq_len\n",
    "        y = torch.tensor(self.labels[idx+self.seq_len], dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "# Modell: LSTM + Dropout + MLP-Head\n",
    "def build_model(input_size=2, hidden_size=32, num_layers=1, dropout=0.2, mlp_hidden=16):  # angepasst\n",
    "    class SOCModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            # LSTM ohne Dropout (voller Informationsfluss)\n",
    "            self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                                batch_first=True, dropout=0.0)\n",
    "            # hidden_size bestimmt die Dim. der LSTM-Ausgabe\n",
    "            # mlp_hidden ist die Größe der verborgenen MLP-Schicht\n",
    "            self.mlp = nn.Sequential(\n",
    "                nn.Linear(hidden_size, mlp_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),   # nur hier Dropout\n",
    "                nn.Linear(mlp_hidden, 1)\n",
    "            )\n",
    "\n",
    "        def forward(self, x, hidden):\n",
    "            out, hidden = self.lstm(x, hidden)\n",
    "            last = out[:, -1, :] # output von jedem und nicht immer den letzten von einer Sequenz / synchronous many to many\n",
    "            soc = self.mlp(last)\n",
    "            return soc.squeeze(-1), hidden\n",
    "    return SOCModel().to(device)\n",
    "\n",
    "# Training mit Mixed Precision und Batch-Fenstern\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "def train_online(epochs=20, lr=1e-3, seq_len=100, batch_size=128):  # vorher 64\n",
    "    train_scaled, df_val, df_test, train_cells, val_cell = load_data()\n",
    "    print(\"Training auf Zellen:\", train_cells)\n",
    "\n",
    "    # Rohdaten-Plots\n",
    "    for name, df in train_scaled.items():\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.plot(df['timestamp'], df['SOC_ZHU'], label=name)\n",
    "        plt.title(f\"Train SOC {name}\")\n",
    "        plt.tight_layout(); plt.savefig(f\"train_{name}_plot.png\"); plt.close()\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(df_val['timestamp'], df_val['SOC_ZHU']); plt.title(\"Val SOC\"); plt.tight_layout(); plt.savefig(\"val_data_plot.png\"); plt.close()\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(df_test['timestamp'], df_test['SOC_ZHU']); plt.title(\"Test SOC\"); plt.tight_layout(); plt.savefig(\"test_data_plot.png\"); plt.close()\n",
    "\n",
    "    model = build_model()\n",
    "    optim = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = StepLR(optim, step_size=5, gamma=0.5)\n",
    "    criterion = nn.MSELoss()\n",
    "    scaler = GradScaler()  # remove deprecated 'device' kwarg\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train()\n",
    "        total_loss, total_steps = 0.0, 0\n",
    "        print(f\"\\n=== Epoch {ep}/{epochs} ===\")\n",
    "        for name, df in train_scaled.items():\n",
    "            print(f\"--> {name}, Schritte: {len(df)-seq_len}\")\n",
    "            ds = WindowedDataset(df, seq_len)\n",
    "            dl = DataLoader(\n",
    "                ds,\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=4,        # erhöht für schnellere Datenvorbereitung\n",
    "                pin_memory=True       # beschleunigt Übertragung auf GPU\n",
    "            )\n",
    "            # TQDM-Balken pro Batch\n",
    "            for x_b, y_b in tqdm(dl, desc=f\"{name} Ep{ep}\", leave=True):\n",
    "                x_b, y_b = x_b.to(device), y_b.to(device)\n",
    "                h0 = torch.zeros(model.lstm.num_layers, x_b.size(0), model.lstm.hidden_size, device=device)\n",
    "                c0 = torch.zeros_like(h0)\n",
    "                optim.zero_grad()\n",
    "                with autocast():\n",
    "                    pred, _ = model(x_b, (h0, c0)) # hier müsste if hidden exists benutzt werden wenn nicht dann h0 und c0 so wie im test\n",
    "                    loss = criterion(pred, y_b)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optim)\n",
    "                clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optim)\n",
    "                scaler.update()\n",
    "                total_loss += loss.item()\n",
    "                total_steps += 1\n",
    "        scheduler.step()\n",
    "        avg_loss = total_loss / total_steps\n",
    "        print(f\"Epoch {ep} abgeschlossen, avg loss={avg_loss:.6f}\")\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), \"best_online_soc.pth\")\n",
    "\n",
    "# Test: Schritt-für-Schritt Streaming-Inferenz ohne Fensterpuffer\n",
    "def test_online():\n",
    "    _, df_val, df_test, train_cells, val_cell = load_data()\n",
    "    print(\"Test auf Zelle:\", val_cell)\n",
    "    model = build_model()\n",
    "    model.load_state_dict(torch.load(\"best_online_soc.pth\", map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    h = torch.zeros(model.lstm.num_layers, 1, model.lstm.hidden_size, device=device)\n",
    "    c = torch.zeros_like(h)\n",
    "    preds, gts = [], []\n",
    "    ds = df_test\n",
    "    # TQDM-Balken im Test\n",
    "    for v, i in tqdm(zip(ds['Voltage[V]'].values, ds['Current[A]'].values),\n",
    "                      total=len(ds), desc=\"Testing\"):\n",
    "        x = torch.tensor([[v, i]], dtype=torch.float32, device=device).view(1,1,2)\n",
    "        pred, (h, c) = model(x, (h, c))\n",
    "        preds.append(pred.item())\n",
    "        gts.append(ds['SOC_ZHU'].iloc[len(preds)-1])\n",
    "    preds, gts = np.array(preds), np.array(gts)\n",
    "    timestamps = df_test['timestamp'].values\n",
    "    mae = np.mean(np.abs(preds - gts)); rmse = np.sqrt(np.mean((preds - gts)**2))\n",
    "    print(f\"Test MAE: {mae:.4f}, RMSE: {rmse:.4f}\")\n",
    "\n",
    "    # Plots wie gehabt\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.plot(timestamps, gts, 'k-', label=\"GT\"); plt.plot(timestamps, preds, 'r-')\n",
    "    plt.title(\"Online Final Test\"); plt.annotate(f\"MAE: {mae:.4f}\\nRMSE: {rmse:.4f}\", xy=(0.01,0.95), xycoords='axes fraction', va='top')\n",
    "    plt.tight_layout(); plt.savefig(\"final_online_plot.png\"); plt.close()\n",
    "    zoom_n = min(50000, len(preds))\n",
    "    for name, seg in [(\"Start\", slice(0, zoom_n)), (\"End\", slice(-zoom_n, None))]:\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.plot(timestamps[seg], gts[seg], 'k-'); plt.plot(timestamps[seg], preds[seg], 'r-')\n",
    "        plt.title(f\"Zoom {name}\"); plt.tight_layout(); plt.savefig(f\"zoom_{name.lower()}_online_plot.png\"); plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_online()\n",
    "    test_online()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
